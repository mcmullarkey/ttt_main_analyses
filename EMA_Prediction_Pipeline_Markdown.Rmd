---
title: "Practicing_EMA_Prediction_Pipeline"
author: "Michael Mullarkey"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: no
      smooth_scroll: no
geometry: margin=0.50in
---

```{r setup, include=FALSE, cache = FALSE}
require("knitr")
## setting working directory
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, include = FALSE)
```

```{r}

## Need to load tensorflow into r (since it's a dependency for the embed package with the umap non-linear dimensionality reduction) before we point to the Python environment

# library(tensorflow)
# tensorflow::install_tensorflow()

## Setting up pyton within Rstudio

## Using these steps https://support.rstudio.com/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio

## For step 2 I used to use conda to make sure I used a later Python off the system path as outlined here, but that caused more headaches so I just follow all steps outlined in the website above instead. https://support.rstudio.com/hc/en-us/articles/360022909454-Best-Practices-for-Using-Python-with-RStudio-Connect

## Here's a video that also walks through the process https://docs.rstudio.com/tutorials/user/using-python-with-rstudio-and-reticulate/

if(!require(reticulate)){install.packages('reticulate')}
library(reticulate)

# Only run this once to create the environment
# conda_create("ttt-reticulate-yml", environment = "ttt_conda_environment.yml")

use_condaenv("ttt-reticulate-yml")

py_config()

# To use for embed package, though mgiht not be necessary

# library(tensorflow)
# tensorflow::install_tensorflow(method = "conda")


```

```{python}

# Checking Python is working within RStudio

1 + 1

```


```{r loading packages}

if(!require(tidymodels)){install.packages('tidymodels')}
library(tidymodels)
if(!require(tidyverse)){install.packages('tidyverse')}
library(tidyverse)
if(!require(skimr)){install.packages('skimr')}
library(skimr)
if(!require(furrr)){install.packages('furrr')}
library(furrr)
if(!require(tictoc)){install.packages('tictoc')}
library(tictoc)
if(!require(heatmaply)){install.packages('heatmaply')}
library(heatmaply)
if(!require(doMC)){install.packages('doMC')}
library(doMC)
if(!require(glue)){install.packages('glue')}
library(glue)
if(!require(stacks)){install.packages('stacks')}
library(stacks)
if(!require(janitor)){install.packages('janitor')}
library(janitor)
if(!require(future)){install.packages('future')}
library(future)
if(!require(reticulate)){install.packages('reticulate')}
library(reticulate)
if(!require(psych)){install.packages('psych')}
library(psych)
if(!require(imputeTS)){install.packages('imputeTS')}
library(imputeTS)
if(!require(timetk)){install.packages('timetk')}
library(timetk)
if(!require(tidyquant)){install.packages('tidyquant')}
library(tidyquant)
if(!require(tsibble)){install.packages('tsibble')}
library(tsibble)
if(!require(feasts)){install.packages('feasts')}
library(feasts)
if(!require(dtw)){install.packages('dtw')}
library(dtw)
if(!require(parallelDist)){install.packages('parallelDist')}
library(parallelDist)
if(!require(pheatmap)){install.packages('pheatmap')}
library(pheatmap)
if(!require(diffdf)){install.packages('diffdf')}
library(diffdf)
# if(!require(tensorflow)){install.packages('tensorflow')}
# library(tensorflow)
# if(!require(keras)){install.packages('keras')}
# library(keras)
# if(!require(embed)){install.packages('embed')}
# library(embed)
if(!require(lubridate)){install.packages('lubridate')}
library(lubridate)
if(!require(ggdist)){install.packages('ggdist')}
library(ggdist)
if(!require(gt)){install.packages('gt')}
library(gt)
if(!require(broom)){install.packages('broom')}
library(broom)
if(!require(glmnet)){install.packages('glmnet')}
library(glmnet)
if(!require(xgboost)){install.packages('xgboost')}
library(xgboost)

## Let's set our number of cores for this document (May differ across computers)

registerDoMC(cores = 7)

```

## Reading in the Data

```{r reading in pre post data}

## Reading in pre/post data for the college students

pre_post_data <- read_csv("cleaned_qualtrics_ttt_phase_1.csv") %>% 
  clean_names()

```

```{r}

pre_post_data

```


```{r reading in ema data}

## Reading in EMA data

ema_data_init <- read_csv("cleaned_lifepak_ttt_phase_1.csv") %>% 
  clean_names()

```

```{r}

# check to see if anyone doesn't have enough data to even approximate a series

too_few_sessions <- ema_data_init %>% 
  count(lifepak_id) %>% 
  filter(n <= 4) %>% 
  print()

## Let's keep everyone with at least half the days (we only lose one person this way)

ema_data_sec <- ema_data_init %>% 
  filter(!lifepak_id %in% too_few_sessions$lifepak_id)

# Confirm they've been dropped

ema_data_sec %>% 
  count(lifepak_id) %>% 
  filter(n <= 4)

```

```{r}

## does anyone have more than 5 observations for any given day?
too_many_obs <- ema_data_sec %>% 
  mutate(day = day(notification_time)) %>% 
  count(lifepak_id, day) %>% 
  filter(n > 5)

## Taking the first response of the two in this case (You could average, but only accepting the first one as valid seems more in line with the data generating process for the other responses, though that's a subjective call)

ema_data_extra <- ema_data_sec %>% 
  mutate(day = day(notification_time)) %>%
  group_by(lifepak_id, day) %>%
  mutate(row = row_number()) %>% 
  filter(row <= 5) %>%
  dplyr::select(-row) %>% 
  ungroup()

# Confirming the later row was dropped for that participant and no one else has more than 5 observations in a day

ema_data_extra %>% 
  count(lifepak_id, day) %>% 
  filter(n > 5)
  
```

# Joining Youth Pre/3 Month CDI to LifePak Data 

```{r}

cdi_data_init <- ema_data_extra %>% 
  left_join(pre_post_data %>% select(yb_life_pak_id, sum_yb_cdi, sum_y3m_cdi), by = c("lifepak_id" = "yb_life_pak_id")) %>% 
  mutate(lifepak_id_int = as.integer(lifepak_id),
         yb_dep_diff = sum_y3m_cdi -sum_yb_cdi)

```
# Looking at How Frequently Sessions Were Completed

```{r}

cdi_data_init %>% 
  group_by(lifepak_id) %>% 
  count(completed_session) %>% 
  filter(completed_session == 1) %>% 
  ungroup() %>% 
  summarize(avg_completed = mean(n), sd_completed = sd(n))

```
# Descriptive Statistics Grouped by Person

```{r}

# cdi_data_init %>% 
#   group_by(lifepak_id) %>% 
#   summarize(mean = mean(bad, na.rm = TRUE), sd = sd(bad, na.rm = TRUE)) %>% 
#   mutate(sd_range = if_else(sd < 15, "SD < 15", "SD > 15")) %>% 
#   ggplot(aes(x = lifepak_id, ydist = "norm", arg1 = mean, arg2 = sd)) +
#   stat_halfeye(position = "dodge") +
#   theme_minimal() +
#   theme(axis.text.x = element_blank()) +
#   ylim(0, 100) +
#   facet_wrap(~sd_range)
  

```

```{r}

# cdi_data_init %>% 
#   group_by(lifepak_id) %>% 
#   summarize(mean = mean(bad, na.rm = TRUE), sd = sd(bad, na.rm = TRUE)) %>% 
#   mutate(sd_range = case_when(
#     sd < 10 ~ "SD < 10", 
#     sd >= 10 & sd < 20 ~"20 > SD > 10",
#     sd >= 20 ~ "SD > 20"
#     )
# ) %>% 
#   ggplot(aes(x = lifepak_id, ydist = distributional::dist_normal(mean, sd))) +
#   stat_halfeye(position = "dodge") +
#   theme_minimal() +
#   theme(axis.text.x = element_blank()) +
#   ylim(0, 100) +
#   facet_wrap(~sd_range)

```
## Plotting Raw Data for Bad by Participant

We can use this as a figure in the main text to illustrate how the distributions are different across participants.

```{r}

cdi_data_init %>% 
  ggplot(aes(x = sad, color = sad)) +
  geom_dots(alpha = 0.6) +
  scale_color_distiller(type = "seq",
                        direction = -1,
                        palette = "Greys") +
  theme_minimal() +
  theme(legend.position = "none",
        plot.title.position = "plot",
        axis.text.y = element_blank(),
        axis.text.x = element_blank()) +
  facet_wrap(~lifepak_id) +
  labs(title = "Distributions of 'Sad' EMA Ratings Grouped by Participant",
    subtitle = "Minimum X-Axis Value is 0 and Maximum X-Axis Value is 100",
    x = "")

ggsave('sad_distribution.jpg')

```

## Creating Table of Summary Statistics for EMA Data by Participant

This table can be part of the supplement.

```{r}

cdi_data_init %>% 
   group_by(lifepak_id) %>% 
   summarize(
     across(
       c(bad,control:movement, sad),
       list(mean = "mean", sd = "sd"),
       na.rm = TRUE,
       .names = "{.col}_{.fn}"
     )
   ) %>% 
  gt(rowname_col = "lifepak_id") %>% 
  tab_stubhead(label = "ID") %>% 
  fmt_number(columns = where(is.numeric),
             decimals = 2) %>% 
  cols_label(bad_mean = "Mean",
             bad_sd = "SD",
             control_mean = "Mean",
             control_sd = "SD",
             energy_mean = "Mean",
             energy_sd = "SD",
             focus_mean = "Mean",
             focus_sd = "SD",
             fun_mean = "Mean",
             fun_sd = "SD",
             interest_mean = "Mean",
             interest_sd = "SD",
             movement_mean = "Mean",
             movement_sd = "SD",
             sad_mean = "Mean",
             sad_sd = "SD") %>% 
  tab_spanner(label = "Bad",
              columns = c(bad_mean, bad_sd)) %>% 
  tab_spanner(label = "Control",
              columns = c(control_mean, control_sd)) %>% 
  tab_spanner(label = "Energy",
              columns = c(energy_mean, energy_sd)) %>% 
  tab_spanner(label = "Focus",
              columns = c(focus_mean, focus_sd)) %>% 
  tab_spanner(label = "Fun",
              columns = c(fun_mean, fun_sd)) %>% 
  tab_spanner(label = "Interest",
              columns = c(interest_mean, interest_sd)) %>% 
  tab_spanner(label = "Movement",
              columns = c(movement_mean, movement_sd)) %>% 
  tab_spanner(label = "Sad",
              columns = c(sad_mean, sad_sd))

```

```{r nesting the ema data for wide format and rsample breaking things up into training and testing}

## Nesting the data so the ema data can be included within a wide format (Helps us break up into training/testing + create custom recipe steps)

nested_ema_data <- cdi_data_init %>% 
  group_by(lifepak_id, yb_dep_diff) %>%
  nest() %>% 
  filter(!is.na(yb_dep_diff)) %>% 
  dplyr::rename(ema_data = data)

## Break into training/testing data

set.seed(33)
# Put 1/2 of the data into the training set, stratify based on the ts_diff outcome 
nested_ema_split <- initial_split(nested_ema_data, prop = 4/5, strata = yb_dep_diff)

# Create data frames for the two sets:
train_nested_ema <- training(nested_ema_split)
test_nested_ema  <- testing(nested_ema_split)

```

```{r creating long form training and testing data}

train_long_ema <- cdi_data_init %>% 
  dplyr::select(lifepak_id:response_no, c(bad, control:movement, sad),lifepak_id_int) %>% 
  filter(lifepak_id %in% train_nested_ema$lifepak_id) %>% 
  dplyr::select(-lifepak_id) %>% 
  group_by(lifepak_id_int) %>% 
  complete(response_no = 1:107) %>%
  dplyr::mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  ),
  response_no = as.integer(response_no)) %>%
  ungroup() %>% 
  print()

# train_long_ema_data_ts_fresh <- train_long_ema %>% 
#   dplyr::select(-contains("dep"))

train_outcome_wide <- train_nested_ema %>% 
  unnest() %>% 
  distinct(lifepak_id, .keep_all = T) %>% 
  ungroup() %>% 
  dplyr::select(yb_dep_diff)

test_long_ema <- cdi_data_init %>% 
  dplyr::select(lifepak_id:response_no, c(bad, control:movement, sad),lifepak_id_int) %>% 
  filter(lifepak_id %in% train_nested_ema$lifepak_id) %>% 
  dplyr::select(-lifepak_id) %>% 
  group_by(lifepak_id_int) %>% 
  complete(response_no = 1:107) %>%
  dplyr::mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  ),
  response_no = as.integer(response_no)) %>%
  ungroup() %>% 
  print()

## Creating the number of cores to use in parallel for tsfresh (Only using physical cores with logical = FALSE)

cores <- detectCores(logical = FALSE) - 1L

```

```{python trying to run tsfresh on the long training data}

## Using https://tsfresh.readthedocs.io/en/latest/text/quick_start.html
## If I try to impute/filter I end up with a different number of rows, so that's not super helpful. Might be names collisons with the example I loaded earlier or something else about how I'm running the Python code I don't understand yet

## Use this page as a guideline for switching back and forth between Pyton and R within RStudio/RMarkdown https://rstudio.github.io/reticulate/articles/r_markdown.html

## Example from the docs works https://tsfresh.readthedocs.io/en/latest/text/quick_start.html

# from tsfresh.examples.robot_execution_failures import download_robot_execution_failures,load_robot_execution_failures
# download_robot_execution_failures()
# timeseries, y = load_robot_execution_failures()
# 
# print(timeseries)
# 
# type(timeseries)
# 
# timeseries.dtypes
# 
from tsfresh import extract_features
# extracted_features = extract_features(timeseries, column_id="id", column_sort="time")

## Trying to apply that to our data

print(r.train_long_ema) # Looks similar as the timeseries data

type(r.train_long_ema) # Has same type as timeseries

r.train_long_ema.dtypes # If we read in this data with the depression outcome as a boolean (which shouldn't be in the data at all) tsfresh won't work

r.train_long_ema.dtypes # So we read in the updated data without the depression outcome as boolean

extracted_features_r = extract_features(r.train_long_ema, column_id="lifepak_id_int", column_sort="response_no")

```

```{r looking at the tsfresh object}

## A function for selecting non-na values

not_all_na <- function(x) {!all(is.na(x))}

# A funciton for selecting non all 0 values

not_all_zero <- function(x) {!all(x == 0)}

tic()
ts_fresh_features <- as_tibble(py$extracted_features_r) %>% 
  clean_names() %>% 
  select_if(not_all_na) %>% 
  select_if(not_all_zero)
toc()

glimpse(ts_fresh_features) 

ts_fresh_id <- ts_fresh_features %>% 
  bind_cols(train_nested_ema %>% ungroup() %>% dplyr::select(lifepak_id)) %>% 
  relocate(lifepak_id, everything())
  

```

```{r writing a function to create dynamic time warp features}

## Need this function for imputation

## Creating function to do necessary imputation

tidy_imputation <- function(.data){

  tidyverse_lags_test <- .data %>%
  dplyr::select(-c(response_no, lifepak_id_int)) %>%
  dplyr::mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  ))

}

## See this blog post https://eiko-fried.com/modeling-idiographic-and-nomothetic-dynamics-of-255-depressed-inpatients/ and this paper https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-020-01867-5
## Also this code https://eiko-fried.com/wp-content/uploads/Code-Hebbrecht-et-al-2020-dtw.txt

## Creating function to generate Dynamic Time Warp distance values for each pair of time series within each person

creating_dtw_distances <- function(.data){

  row_names <- .data %>% 
    dplyr::select(-lifepak_id_int) %>% 
    names()
  column_names <- .data %>% 
    dplyr::select(-lifepak_id_int) %>% 
    names()
  
  distance <- parDist(.data %>% dplyr::select(-lifepak_id_int) %>%  as.matrix(), method = "dtw", 
    window.type = "sakoechiba", window.size = 2, step.pattern = "symmetricP0")
  
  distance <- as.matrix(distance)
  distance <- distance[c(1:8),c(1:8)]
  
  colnames(distance) <- column_names
  
  distance <- as_tibble(distance) %>% 
    mutate(var_names = row_names) %>% 
    relocate(var_names, everything())
}

# heatmap_ema <- function(dist){
#     pheatmap(dist %>% as.matrix(), 
#          display_numbers = TRUE, border_color = FALSE, 
#          clustering_method = "ward.D2", treeheight_row = 0, 
#          treeheight_col = 0, color = colorRampPalette(brewer.pal(n = 7, name = "RdYlBu"))(100), 
#          number_format = "%.0f", legend = FALSE)
# }
# 
# heatmap_ema(distance)

## Creating function to pivot the DTW results wider to create the features + renaming them using the names we dynamically generated

pivoting_dtw_results_wider <- function(.data){
.data %>% 
  as_tibble() %>% 
  pivot_wider(
    names_from = var_names,
    values_from = c(bad:sad)
  )
}

## Creating duplicate names for DTW features to help remove them from the feature set later (Since they're all 0)

exclude_dtw_same_var_features <- function(.data){
  
  ready_for_dup_name <- .data %>% 
    dplyr::select(where(is.numeric)) %>% 
    names()

dupe_df <- enframe(ready_for_dup_name) %>% 
  mutate(first = word(value, 1, sep = "_"),
         second = word(value, -1, sep = "_")) %>% 
  filter(first == second)
  
  col_names_dup <- glue("{dupe_df$first}_{dupe_df$first}")

 .data <- .data %>%
  dplyr::select(-all_of(col_names_dup))
}

## Do it once, but then accidentally do the whole thing

# Have to create id only column data frame to bind back with this function at the end

lags_id_only <- train_nested_ema %>% 
  dplyr::select(-ema_data, -contains("yb"))

# Now the function

tic()
train_dtw_stats <-cdi_data_init %>% 
  dplyr::select(lifepak_id:response_no, c(bad, control:movement, sad),lifepak_id_int) %>% 
  filter(lifepak_id %in% train_nested_ema$lifepak_id) %>% 
  dplyr::select(-lifepak_id) %>% 
  group_by(lifepak_id_int) %>% 
  complete(response_no = 1:107) %>%
  dplyr::mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  ),
  response_no = as.integer(response_no)) %>%
  ungroup() %>% 
  as_tsibble(key = lifepak_id_int, index = response_no) %>% 
  group_by(lifepak_id_int) %>% 
  tidy_imputation() %>% 
  dplyr::select(where(is.numeric)) %>% 
  ungroup() %>%
  as_tibble() %>% 
  dplyr::select(-response_no) %>% 
  group_split(lifepak_id_int) %>% 
  map(creating_dtw_distances) %>%
  map_dfr(pivoting_dtw_results_wider) %>% 
  bind_cols(lags_id_only) %>% 
  ungroup() %>% 
  exclude_dtw_same_var_features() %>% 
  relocate(lifepak_id, everything()) %>% 
  dplyr::select(-starts_with("yb_dep_diff"))
toc()

```

```{r running all preprocessing once}

## Merge tsfresh with dynamic time warp features

tic()
train_data_preprocess <- ts_fresh_id %>% 
  left_join(train_dtw_stats, by = "lifepak_id") %>%
  mutate(across(everything(), ~ifelse(is.nan(.x), NA, .x))) %>% 
  mutate(across(everything(), ~ifelse(is.infinite(.x), NA, .x))) %>% 
  bind_cols(train_outcome_wide)
toc()

## Using prep and bake with recipes to preprocess data (Getting rid of near zero variance features, normalizing all variables (necessary preprocessing for the neural network autoencoder), and using K nearest neighbors to impute missing predictor data

## Maybe add a Box_Cox Step? Should visualize skew/non skew of predictors before and after to check

rec_ema <- 
  recipes::recipe(yb_dep_diff~ ., data = train_data_preprocess) %>% 
  update_role(lifepak_id, new_role = "id") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_impute_knn(all_predictors())

tic()
set.seed(33)
train_imp_nzv <- rec_ema %>% 
  prep(verbose = TRUE) %>% 
  bake(new_data = train_data_preprocess)
toc()

```

```{r creating recipe for running a predictive model with pca generated features}

## Recipes don't like list columns, but I may be able to hack around it https://github.com/tidymodels/recipes/issues/402

rec_ema_pca <- 
  recipes::recipe(yb_dep_diff ~ ., data = train_imp_nzv) %>% 
  update_role(lifepak_id, new_role = "id") %>% 
  step_pca(all_predictors(), threshold = 0.95)

## Could also work on making threshold tunable

```

```{r creating recipe for running a predictive model with the raw features}

## Recipes don't like list columns, but I may be able to hack around it https://github.com/tidymodels/recipes/issues/402

rec_ema_raw <- 
  recipes::recipe(yb_dep_diff ~ ., data = train_imp_nzv) %>% 
  update_role(lifepak_id, new_role = "id")

```

```{r creating a recipe for running a predictive model with the umap features}

## This is supervised, non-linear dimensionality reduction without needing to use an autoencoder

# rec_ema_umap <- 
#   recipes::recipe(dep_deteriorate ~ ., data = train_data_with_summ_stats_imp_nzv) %>% 
#   update_role(id, new_role = "id") %>% 
#   step_umap(all_predictors(), outcome = vars(dep_deteriorate), num_comp = 5)

```


```{r testing whether the preliminary recipe works within a workflow}

# Create a random forest model

rf_mod <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

elnet_mod <- linear_reg(penalty = 0.99, mixture = 0.99) %>% 
  set_engine("glmnet")

xg_mod <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

# Create a list of preprocessing recipes

base_model_rec_list <- list(rec_ema_pca, rec_ema_raw)

# Create a list of models

base_model_mod_list <- list(rf_mod, elnet_mod, xg_mod)

# Create data frame that matches all possible combinations of recipes and models

rec_mods_df <- expand_grid(base_model_rec_list, base_model_mod_list) 

# Combining each recipe into a tidymodels workflows using a map function

base_model_wfs <- map2(rec_mods_df$base_model_rec_list, rec_mods_df$base_model_mod_list, ~{
  
  ema_autoencoder_rf_wf <- workflow() %>% 
  add_recipe(.x) %>% 
  add_model(.y)
}
)

# Create as list of dataframes we'll be using to fit these models

base_model_data_list <- list(train_imp_nzv, train_imp_nzv, train_imp_nzv, train_imp_nzv, train_imp_nzv, train_imp_nzv)

# Fitting each model once to make sure they run, and they do

base_model_one_time_fit <- map2(base_model_wfs, base_model_data_list, ~{
  
tic()
ema_rf_wf_fit <- fit(.x, data = .y)
toc()
ema_rf_wf_fit
  
})

```

```{r}

## Fit resamples is having a hard time running these models (no errors but just continuing trying to compute for long periods of time without actually completing the computation). Might have something to do with this issue: https://github.com/tidymodels/tune/issues/206

base_model_fit_all_rs <- map2(base_model_wfs, base_model_data_list, ~{
  
registerDoMC(cores = 7)

set.seed(33)
folds_ema_pred <- vfold_cv(.y, v = 4, repeats = 10, strata = yb_dep_diff)

## Run the CV models here
keep_pred <- control_resamples(save_pred = TRUE)
tic()
set.seed(33)
rf_fit_rs <- 
  .x %>% 
  fit_resamples(folds_ema_pred, control = keep_pred)
toc()
rf_fit_rs

} 
)

## Get metrics here

base_model_metrics_all_rs <- map(base_model_fit_all_rs, ~{
  
  rsq_metric <- .x %>% 
  collect_metrics(summarize = TRUE)

}) %>% 
  print()

## Saving the predictions/actual differences here

base_model_preds_all_rs <- map(base_model_fit_all_rs, ~{
  
  rsq_metric <- .x %>% 
  collect_predictions(summarize = TRUE)

}) %>% 
  print()

# base_model_plots_all_rs <- map(base_model_fit_all_rs, ~{
#   
#   auc <- .x %>% 
#   collect_predictions(summarize = TRUE) %>% 
#   roc_curve(truth = dep_deteriorate, .estimate = .pred_TRUE) %>% 
#   autoplot()
# 
# }) %>% 
#   print()

```

## THE CODE CURRENTLY IN USE ENDS HERE

```{r getting training data ready for FsNet Autoencoder}

x_train_tbl <- train_data_with_summ_stats_imp_nzv %>% 
  dplyr::select(-c(id_ema,dass_dep_deteriorate_or_not)) %>% 
  as.matrix()
# y_train_tbl <- train_data_with_summ_stats_imp_nzv %>% 
#   dplyr::select(dass_dep_deteriorate_or_not) %>%
#   as.matrix()
y_train_vec <- train_data_with_summ_stats_imp_nzv %>% 
  dplyr::select(dass_dep_deteriorate_or_not) %>% pull()
id_train_vec <-  train_data_with_summ_stats_imp_nzv %>% 
  dplyr::select(id_ema) %>% pull()

```

```{python running FsNet as deep learning non linear dimension reduction}

from __future__ import print_function

import math
import keras
from keras import backend as K
#from keras.datasets import mnist
from keras.models import Sequential, Model
from keras.layers import Layer, Dense, Dropout, Input, LeakyReLU
from keras.layers.core import Activation
from keras.optimizers import RMSprop
from keras.initializers import Constant, glorot_normal
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
import numpy as np
import scipy.io as spio
import random
import matplotlib.pyplot as plt
import sys
import pandas as pd

dp=sys.argv[1]
ds=sys.argv[2]
h_size=int(sys.argv[3])
nfeat=int(sys.argv[4])
rd="eta100/"
datafile=dp+ds

num_exp=20
num_epochs=6400
bins=10
batch_size=8
start_temp=10.0
min_temp=0.01
lossWeights = {"recon":100, "classacc":1}
losses = {"recon": "mean_squared_error", "classacc": "categorical_crossentropy",}
opt=RMSprop(lr=0.001, decay=0.001/num_epochs)

cacc=np.zeros(num_epochs)
acc=np.zeros(num_epochs)
closs=np.zeros(num_epochs)
loss=np.zeros(num_epochs)
cmi=0
mi=0


def calc_MI(X,Y,bins):
   c_XY = np.histogram2d(X,Y,bins)[0]
   c_X = np.histogram(X,bins)[0]
   c_Y = np.histogram(Y,bins)[0]
   H_X = shan_entropy(c_X)
   H_Y = shan_entropy(c_Y)
   H_XY = shan_entropy(c_XY)
   mi1 = H_X + H_Y - H_XY
   return mi1

def shan_entropy(c):
    c_normalized = c / float(np.sum(c))
    c_normalized = c_normalized[np.nonzero(c_normalized)]
    H = -sum(c_normalized* np.log2(c_normalized))  
    return H

def MI(S):
  bins = 10
  n = S.shape[1]
  mis=0
  count=0
  for ix in np.arange(n):
    for jx in np.arange(ix+1,n):
        mis = mis+calc_MI(S[:,ix], S[:,jx], bins)
        count=count+1
  mis=mis/count
  return mis


class tinyLayerE(Layer):
  def __init__(self, output_dim, u, bins, start_temp=10.0, min_temp=0.1, alpha=0.99999, **kwargs):
    self.output_dim=output_dim
    self.u=K.constant(u)
    self.start_temp = start_temp
    self.min_temp = K.constant(min_temp)
    self.alpha = K.constant(alpha)
    super(tinyLayerE, self).__init__(**kwargs)
	
  def build(self,input_shape):
    self.temp = self.add_weight(name = 'temp', shape = [], initializer = Constant(self.start_temp), trainable = False)
    #self.sf = self.add_weight(name = 'sf', shape = [], initializer = Constant(1500), trainable = True)
    self.tinyW=self.add_weight(name='tinyW', shape=(bins,self.output_dim), initializer='uniform', trainable=True)
    super(tinyLayerE,self).build(input_shape)
	
  def call(self, X, training = None):
    al=K.softmax(K.dot(self.u,self.tinyW))
    al=K.transpose(al) #al=K.transpose(al*K.one_hot(K.argmax(al),al.shape[1]))
    #al=(np.sqrt(2.0/(al.shape[0].value*al.shape[1].value)))*((al-K.mean(al))/K.std(al))
    logits=K.log(10*K.maximum(K.minimum(al,0.9999999),K.epsilon()))
    uniform = K.random_uniform(logits.shape, K.epsilon(), 1.0)
    gumbel = -K.log(-K.log(uniform))
    temp = K.update(self.temp, K.maximum(self.min_temp, self.temp * self.alpha))
    noisy_logits = (logits+gumbel) / temp
    samples = K.softmax(noisy_logits)
    discrete_logits = K.one_hot(K.argmax(logits), logits.shape[1])
    self.logits=samples
    dl = np.zeros(self.logits.shape)
    p = K.get_value(self.logits)
    for i in range(dl.shape[0]):
      ind = np.argmax(p, axis=None)
      x=ind//dl.shape[1]
      y=ind%dl.shape[1]
      dl[x][y]=1
      p[x]=-np.ones(dl.shape[1])
      p[:,y]=-np.ones(dl.shape[0])
      discrete_logits = K.one_hot(K.argmax(K.variable(dl)), dl.shape[1])
    self.selections = K.in_train_phase(samples, discrete_logits, training)
    Y = K.dot(X, K.transpose(self.selections))
    return Y

  def compute_output_shape(self, input_shape):
    return (input_shape[0], self.output_dim)

class tinyLayerD(Layer):
  def __init__(self, output_dim, u, bins, **kwargs):
    self.output_dim=output_dim
    self.u=K.constant(u)
    super(tinyLayerD, self).__init__(**kwargs)
  def build(self,input_shape):
    self.tinyW=self.add_weight(name='tinyW', shape=(bins, input_shape[1]), initializer='uniform', trainable=True)
    super(tinyLayerD,self).build(input_shape)
	
  def call(self, x):
    weights=K.transpose(K.tanh(K.dot(self.u,self.tinyW)))
    return K.dot(x,weights)
	
  def compute_output_shape(self, input_shape):
    return (input_shape[0], self.output_dim)


# the data, split between train and test sets
data=spio.loadmat(datafile)
X=data['X']
Y=data['Y']
Y = to_categorical(Y)

#Normalization to N(0,1)ds123

X=np.delete(X,np.where(np.std(X,axis=0)==0),axis=1)
for i in range(X.shape[1]):
  if np.max(X[:,i])!=0:
    X[:,i]=X[:,i]/np.max(np.absolute(X[:,i]))
    mu_Xi=np.mean(X[:,i])
    std_Xi=np.std(X[:,i])
    X[:,i]=X[:,i]-mu_Xi
    if std_Xi!=0:
      X[:,i]=X[:,i]/std_Xi

for ii in range(0,num_exp):
  idx=random.sample(range(0,X.shape[0]),round(X.shape[0]*0.5))
  x_train=X[idx,:]
  y_train=Y[idx,:]
  x_test=np.delete(X,idx,0)
  y_test=np.delete(Y,idx,0)
  x_train = np.reshape(x_train, (len(x_train), -1))
  x_test = np.reshape(x_test, (len(x_test), -1))

  u_train=np.zeros([x_train.shape[1],bins],dtype=float)
  for i in range(0,x_train.shape[1]):
    hist=np.histogram(x_train[:,i],bins)
    for j in range(0,bins):
      u_train[i,j]=hist[0][j]*0.5*(hist[1][j]+hist[1][j+1])

  steps_per_epoch = (len(x_train) + batch_size - 1) // batch_size
  alpha = math.exp(math.log(min_temp / start_temp) / (num_epochs * steps_per_epoch))


  ################################
  # FsNet
  ################################

  inp1=Input(shape=(x_train.shape[1],))
  x=tinyLayerE(nfeat,u_train,bins,start_temp, min_temp, alpha, name = 'tinyLayerE')(inp1)
  x = Dense(h_size*4)(x)
  x = LeakyReLU(0.2)(x)
  x = Dropout(0.2)(x)
  x = Dense(h_size*2)(x)
  x = LeakyReLU(0.2)(x)
  x = Dropout(0.2)(x)
  x = Dense(h_size)(x)
  x = LeakyReLU(0.2)(x)
  x = Dropout(0.2)(x)
  x1 = Dense(h_size*2)(x)
  x1 = LeakyReLU(0.2)(x1)
  x1 = Dropout(0.2)(x1)
  x1 = Dense(h_size*4)(x1)
  x1 = LeakyReLU(0.2)(x1)
  x1 = Dropout(0.2)(x1)
  x1 = tinyLayerD(x_train.shape[1],u_train,bins,name = 'recon')(x1)
  x2 = Dense(y_train.shape[1])(x)
  x2 = Activation("softmax", name="classacc")(x2)
  model = Model(inputs=inp1, outputs=[x1, x2])
  model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights, metrics=["accuracy","mse"])
  history = model.fit(x_train, {"recon": x_train, "classacc": y_train}, validation_data=(x_test, {"recon": x_test, "classacc": y_test}), epochs=num_epochs, verbose=1)
  probabilities = K.get_value(K.softmax(model.get_layer('tinyLayerE').logits))
  dl=np.zeros(model.get_layer('tinyLayerE').logits.shape)
  p=K.get_value(model.get_layer('tinyLayerE').logits)
  for j in range(dl.shape[0]):
    ind=np.argmax(p,axis=None)
    x=ind//dl.shape[1]
    y=ind%dl.shape[1]
    dl[x][y]=1
    p[x]=-np.ones(dl.shape[1])
    p[:,y]=-np.ones(dl.shape[0])

  indices = K.get_value(K.argmax(dl))

  hist_df = pd.DataFrame(history.history)
  hist_csv_file = rd+ds+"_"+str(nfeat)+"_"+str(ii)+"_history.csv"
  with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)
  spio.savemat(rd+ds+"_"+str(nfeat)+"_"+str(ii)+'_indices.mat', {'indices': indices})

```

```{r running autoencoder function}

## Having to mix together resources to figure out the autoencoder. This youtube video https://www.youtube.com/watch?v=oDhpIDBQSzw and this free textbook on supervised machine learning for text are what I'm using right now

# Now need to convert this data into a format the autoencoder can understand https://tensorflow.rstudio.com/guide/tfdatasets/introduction/ was hwere I looked initially but now using this https://colorado.rstudio.com/rsc/churn/modeling/tensorflow-w-r.nb.html

## Trying to use this https://stackoverflow.com/questions/44591138/making-neural-network-training-reproducible-using-rstudios-keras-interface

## Maybe we can use this approach from Jason/Chris on low sample size high dimensional data https://pdfs.semanticscholar.org/76f7/55fed7bf1cea8dad35c16bf518eab158c13e.pdf

running_autoencoder_and_rf_rs <- function(.data){

x_train_tbl <- .data %>% 
  dplyr::select(-c(id_ema,dass_dep_deteriorate_or_not)) %>% 
  as.matrix()
# y_train_tbl <- train_data_with_summ_stats_imp_nzv %>% 
#   dplyr::select(dass_dep_deteriorate_or_not) %>%
#   as.matrix()
y_train_vec <- .data %>% 
  dplyr::select(dass_dep_deteriorate_or_not) %>% pull()
id_train_vec <-  .data %>% 
  dplyr::select(id_ema) %>% pull()

# Now we try to make the autoencoder happen based on https://doi.org/10.1016/j.jad.2020.12.086 and https://www.datatechnotes.com/2020/02/how-to-build-simple-autoencoder-with-keras-in-r.html

autoencoder <- keras_model_sequential() %>% 
  layer_dense(units = ncol(x_train_tbl), activation = "relu", input_shape = ncol(x_train_tbl)) %>% 
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 2000, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 1000, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 500, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 100, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 25, activation = "relu", name = "features") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 100, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 500, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 1000, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 2000, activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
  layer_dense(units = ncol(x_train_tbl), activation = "relu") %>%
  layer_activation_leaky_relu() %>% 
compile(
  loss = "mean_squared_error",
  optimizer = "rmsprop"
)

summary(autoencoder)

## Would be nice to get AUCs at some point for Keras models, though not neccessary here. Here's a custom AUC function for keras in R that might work https://github.com/rstudio/keras/issues/319

## This creates the full autoencoder model

tic()
autoencoder_fit <- autoencoder %>% 
  fit(
    x = x_train_tbl,
    y = x_train_tbl,
    epochs = 100,
    validation_split = 0.75,
    callbacks = callback_early_stopping(patience = 25)
  )
toc()

## This creates the chopped in half version of the autoencoder model that will let us extract the features

feature_extraction_model <- keras_model(inputs = autoencoder$input,
                                        outputs = get_layer(autoencoder, "features")$output)

## Now have to save the model and weights https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_save_and_restore/

feature_extraction_model %>% save_model_hdf5("my_model.h5")

## And load it into a new model

new_model <- load_model_hdf5("my_model.h5")

## This actually extracts the features from the original data
train_data_with_autoencoder_features <- predict(feature_extraction_model, x_train_tbl) %>% 
  as_tibble() %>% 
  bind_cols(tibble(dass_dep_deteriorate_or_not = y_train_vec, id_ema = id_train_vec)) # Likely don't need this step in the final since I'll just be feeding all the predictors into this autoencoder step and they'll come back as predictors bound to the original data

## Now creating a recipe to predict the outcome of interest with autoencoder features

rec_ema_neural_net <- 
  recipes::recipe(dass_dep_deteriorate_or_not ~ ., data = train_data_with_autoencoder_features) %>% 
  update_role(id_ema, new_role = "id")

rf_mod <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

ema_autoencoder_rf_wf <- workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(rf_mod)

registerDoMC(cores = 7)

set.seed(33)
folds_ema_pred <- vfold_cv(train_data_with_autoencoder_features, v = 4, repeats = 10, strata = dass_dep_deteriorate_or_not)

## Run the CV models here
keep_pred <- control_resamples(save_pred = TRUE)
tic()
set.seed(33)
rf_fit_rs <- 
  ema_autoencoder_rf_wf %>% 
  fit_resamples(folds_ema_pred, control = keep_pred)
toc()

## We can get all the metrics here (This isn't working right now, is storing the collect metrics function rather than the output)
  
auc_val <- rf_fit_rs %>% 
  collect_predictions(summarize = TRUE) %>% 
  roc_auc(truth = dass_dep_deteriorate_or_not, .estimate = .pred_1)

metrics_and_autoencoder_model <- list(new_model, auc_val)

return(metrics_and_autoencoder_model)

}

## Now run it once 

tic()
test_autoencoder_rf_rs_one_run <- train_data_with_summ_stats_imp_nzv %>% 
  running_autoencoder_and_rf_rs()
toc()

```

```{r running autoencoder process 100 times to look at range and averages of AUC}

## This takes ~35 minutes running on 7 cores (Don't want to parallelize map here since we're already using many cores to run the model within the map)

tic()
samples <- 100
set.seed(33)
one_hundred_runs_of_autoencoder_fit <- purrr::map(1:samples,
                        ~ running_autoencoder_and_rf_rs(train_data_with_summ_stats_imp_nzv))
toc()

## This map cheat sheet was helpful for finding the flatten and keep arguments (Trying to only look at the data frame elements of the resulting list to look at the AUCs) https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_purrr.pdf

flattened_list_dfs <- flatten(one_hundred_runs_of_autoencoder_fit) %>% 
  keep(is.data.frame)

vec_of_auc <- map_dbl(.x = flattened_list_dfs, ~{
    
    .x %>% 
      dplyr::select(.estimate) %>% 
      deframe()
    
  })

psych::describe(vec_of_auc)

## Finding which version of the model gave the maximum AUC

which.max(vec_of_auc)

## Creating a list of only the models/model weights

flattened_list_dfs_models <- flatten(one_hundred_runs_of_autoencoder_fit) %>% 
  discard(is.data.frame)

## Plucking the model with the max AUC from resampling that could be applied to new data

best_model_weights <- pluck(flattened_list_dfs_models, which.max(vec_of_auc))

best_model_weights %>% save_model_hdf5("best_autoencoder_auc.h5")

```

```{r need to do all the preprocessing for the test set starting with summary statistics}

plan(multisession, workers = 7)

## Probably need to do some more presprocessing as of now. For example, does the test set already have id_ema or just id?

tic()
test_data_with_summ_stats <- test_nested_ema_data %>% 
  mutate(ema_summ_stats = future_map(ema_data, get_summ_stats_ema_data)) %>% # Will turn into step_mutate
  mutate(ema_quantile_stats = future_map(ema_data, get_quantiles_ema_data)) %>%  # Will turn into step_mutate
  mutate(ema_spec_stats = future_map(ema_data, do_spec_analysis)) %>%
  unnest(c(ema_summ_stats, ema_quantile_stats, ema_spec_stats)) %>% # Will need to create a custom step function
  distinct(id_ema, .keep_all = T) %>% # Will need to create a custom step function (Filter exists as a step, but group_by doesn't so can't shortcut)
  dplyr::select(-ema_data) %>% # Will turn into step_rm
  relocate(id_ema, dass_dep_deteriorate_or_not, everything())
toc()# Only for viewing here, won't be in final recipe
# step_BoxCox(all_predictors()) %>% # Built in, will help make predictors more symmetrical
# step_normalize(all_predictors) %>% # Built in, will normalize all predictors for the autoencoder
# add_role(all_predictors(), new_role = "non_neural_net_predictors") %>% # Giving role to these predictors to facilitate removal post-autoencoder
# step_autoencoder(all_predictors()) %>%  # Will need to create a custom step function
# step_rm(role = "non_neural_net_predictors") # Removing all non autoencoder generated predictors

```

```{r creating acf lags for test set}

## Creating test set ACF Features

test_data_with_acf_stats <- map_dfc(lag_stats_names, ~{
  
tidyverse_lags_test_reduced_acf <- ema_data_with_dep_outcome_num_resp %>%
  filter(id_ema %in% test_nested_ema_data$id_ema) %>% 
  group_by(id_ema) %>% 
  complete(time = 1:56) %>% # Needed to make sure each person has the same number of timepoints https://aosmith.rbind.io/2018/06/27/uneven-grouped-autocorrelation/#problems-with-naively-using-acf
  ungroup() %>% 
  as_tsibble(key = id_ema, index = time) %>% 
  group_by(id_ema) %>% 
  tidy_imputation() %>% 
  ACF(.data[[.x]], lag_max = 10) %>% 
  group_split(id_ema) %>%  # This creates a list of dataframes by id, since pivot_wider won't work given it would create redundant column names
  map_dfr(pivoting_acf_results_wider) %>% 
  create_acf_names(var_name_input = .x)
  
}) %>% 
  rename(temp_id = id_ema...1) %>% 
  dplyr::select(-contains("id_ema")) %>% 
  rename(id_ema = temp_id)
```

```{r creating pacf lags for test set}

test_data_with_pacf_stats <- map_dfc(lag_stats_names, ~{
  
tidyverse_lags_test_reduced_pacf <- ema_data_with_dep_outcome_num_resp %>%
  filter(id_ema %in% test_nested_ema_data$id_ema) %>% 
  group_by(id_ema) %>% 
  complete(time = 1:56) %>% # Needed to make sure each person has the same number of timepoints https://aosmith.rbind.io/2018/06/27/uneven-grouped-autocorrelation/#problems-with-naively-using-acf
  ungroup() %>% 
  as_tsibble(key = id_ema, index = time) %>% 
  group_by(id_ema) %>% 
  tidy_imputation() %>% 
  PACF(.data[[.x]], lag_max = 10) %>% 
  group_split(id_ema) %>%  # This creates a list of dataframes by id, since pivot_wider won't work given it would create redundant column names
  map_dfr(pivoting_pacf_results_wider) %>% 
  create_pacf_names(var_name_input = .x)
  
}) %>% 
  rename(temp_id = id_ema...1) %>% 
  dplyr::select(-contains("id_ema")) %>% 
  rename(id_ema = temp_id)

```

```{r creating feasts package automatic features for test set}

test_data_with_auto_stats <- map_dfc(lag_stats_names, ~{
  
tidyverse_lags_test_reduced_auto <- ema_data_with_dep_outcome_num_resp %>%
  filter(id_ema %in% test_nested_ema_data$id_ema) %>% 
  group_by(id_ema) %>% 
  complete(time = 1:56) %>% # Needed to make sure each person has the same number of timepoints https://aosmith.rbind.io/2018/06/27/uneven-grouped-autocorrelation/#problems-with-naively-using-acf
  ungroup() %>% 
  as_tsibble(key = id_ema, index = time) %>% 
  group_by(id_ema) %>% 
  tidy_imputation() %>% 
  features(.data[[.x]], feature_set(pkgs = "feasts")) %>% 
  dplyr::select(-acf10, -acf1,-pacf5) %>% 
  create_auto_feature_names(var_name_input = .x)
  
}) %>% 
  rename(temp_id = id_ema...1) %>% 
  dplyr::select(-contains("id_ema")) %>% 
  rename(id_ema = temp_id)

```

```{r creating dynamic time warp features for test set}

# Have to create id only column data frame to bind back with this function at the end

tidyverse_test_lags_id_only <- test_nested_ema_data %>% 
  dplyr::select(-ema_data, -dass_dep_deteriorate_or_not)

tic()
test_data_with_dtw_stats <- ema_data_with_dep_outcome_num_resp %>%
  filter(id_ema %in% test_nested_ema_data$id_ema) %>% 
  group_by(id_ema) %>% 
  complete(time = 1:56) %>% # Needed to make sure each person has the same number of timepoints https://aosmith.rbind.io/2018/06/27/uneven-grouped-autocorrelation/#problems-with-naively-using-acf
  ungroup() %>% 
  as_tsibble(key = id_ema, index = time) %>% 
  group_by(id_ema) %>% 
  tidy_imputation() %>% 
  dplyr::select(where(is.numeric)) %>% 
  ungroup() %>%
  as_tibble() %>% 
  dplyr::select(-time) %>% 
  group_split(id_ema) %>% 
  map(creating_dtw_distances) %>%
  map_dfr(pivoting_dtw_results_wider) %>% 
  exclude_dtw_same_var_features() %>% 
  bind_cols(tidyverse_test_lags_id_only) %>% 
  relocate(id_ema, contains("dass"), everything()) %>% 
  dplyr::select(-contains("dass"))
toc()

```

```{r creating weekday weekend features for the test set}


# Get the data

test_data_with_date_df <- ema_data_with_dep_outcome_num_resp %>%
  filter(id_ema %in% test_nested_ema_data$id_ema) %>% 
  complete(time = 1:56)

# Add time series signature
recipe_date_timeseries <- recipe(dass_dep_deteriorate_or_not ~ ., data = test_data_with_date_df) %>%
    step_timeseries_signature(day) %>% 
    step_nzv(all_predictors())

## Now breaking out summary statistics by weekday vs. weekend

set.seed(33)
test_data_with_date_structure <- recipe_date_timeseries %>% 
  prep(verbose = TRUE) %>% 
  bake(new_data = test_data_with_date_df) %>% 
  dplyr::select(-contains("dass")) %>% 
  mutate(weekend = case_when(
    
    day_wday.lbl == "Monday" | day_wday.lbl == "Tuesday" |day_wday.lbl == "Wednesday" | day_wday.lbl == "Thursday" | day_wday.lbl == "Friday" ~ 0,
    day_wday.lbl == "Saturday" | day_wday.lbl == "Sunday" ~ 1
    
  )) %>% 
  group_split(weekend) %>% 
  map(get_summ_stats_ema_data_long)

date_df_names <- test_data_with_date_structure[[1]] %>% 
  names() 

col_names_date_df_weekday <- glue("{date_df_names}_weekday")

col_names_date_df_weekend <- glue("{date_df_names}_weekend")


names(test_data_with_date_structure[[1]]) <- col_names_date_df_weekday
names(test_data_with_date_structure[[2]]) <- col_names_date_df_weekend

test_data_with_weekday_weekend_initial_features <- map_dfc(test_data_with_date_structure, ~.x) %>% 
  rename(id_temp = id_ema_weekday) %>% 
  dplyr::select(-contains("id_ema")) %>% 
  rename(id_ema = id_temp)

```

```{r merging all preprocessed data for test set and removing near zero variance columns plus imputing missing data}

## Merge lags with other datasets

test_data_preprocessing <- test_data_with_summ_stats %>% 
  left_join(test_data_with_acf_stats, by = "id_ema") %>%
  left_join(test_data_with_pacf_stats, by = "id_ema") %>%
  left_join(test_data_with_auto_stats, by = "id_ema") %>%
  left_join(test_data_with_dtw_stats, by = "id_ema") %>%
  left_join(test_data_with_weekday_weekend_initial_features, by = "id_ema") %>%
  mutate(across(everything(), ~ifelse(is.nan(.x), NA, .x))) %>% 
  mutate(across(everything(), ~ifelse(is.infinite(.x), NA, .x)))

## We need to retain the same columns as in the training data in the test data to run the same autoencoder. I used this website as a resource for selecting variables based on the variables in another dataframe https://tidyselect.r-lib.org/reference/all_of.html

vars_in_train_data <- names(train_data_with_summ_stats_imp_nzv)

test_data_preprocessing <- test_data_preprocessing %>% 
  dplyr::select(all_of(vars_in_train_data))

## Using prep and bake with recipes to preprocess data (Getting rid of near zero variance features, normalizing all variables (necessary preprocessing for the neural network autoencoder), and using K nearest neighbors to impute missing predictor data

rec_ema_test_for_autoencoder <- 
  recipes::recipe(dass_dep_deteriorate_or_not ~ ., data = test_data_preprocessing) %>% 
  update_role(id_ema, new_role = "id") %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_knnimpute(all_predictors())

tic()
set.seed(33)
test_data_with_summ_stats_imp_nzv <- rec_ema_test_for_autoencoder %>% 
  prep(verbose = TRUE) %>% 
  bake(new_data = test_data_preprocessing)
toc()

## Trying to diagnose which columns are removed if we do near zero variance step

# compare <- janitor::compare_df_cols(train_data_with_summ_stats_imp_nzv, test_data_with_summ_stats_imp_nzv) %>% 
#   slice(1000:1872)

test_data_preprocessing_for_inclusion <- test_data_preprocessing %>% 
  dplyr::select(anger_1_median_weekday, anger_1_mode, anx_1_pp_pvalue, dep_2_mode, dep_2_mode_weekday, dep_2_pp_pvalue,
                stress_2_pp_pvalue, time_occupied_covid_pp_pvalue, time_soc_contact_pp_pvalue, time_think_health_covid_pp_pvalue) %>% 
  mutate(dep_2_pp_pvalue = ifelse(is.na(dep_2_pp_pvalue), 0.0100000, dep_2_pp_pvalue),
         stress_2_pp_pvalue = ifelse(is.na(stress_2_pp_pvalue), 0.0100000, stress_2_pp_pvalue)
         )

test_data_with_summ_stats_imp_nzv_complete <- test_data_with_summ_stats_imp_nzv %>% 
  bind_cols(test_data_preprocessing_for_inclusion) %>% 
  rename(id_temp = id_ema...1) %>% 
  dplyr::select(-contains("id_ema")) %>% 
  rename(id_ema = id_temp) %>% 
  dplyr::select(-dass_dep_deteriorate_or_not...1864) %>% 
  rename(dass_dep_deteriorate_or_not = dass_dep_deteriorate_or_not...1862) %>% 
  relocate(all_of(vars_in_train_data)) ## Puts all the columns in the same order as the training data, which is important for the autoencoder
  

```

```{r creating features according to autoencoder model with max AUC in test data}

## Setting up code necessary to fit saved model to training data

x_train_tbl <- train_data_with_summ_stats_imp_nzv %>% 
  dplyr::select(-c(id_ema,dass_dep_deteriorate_or_not)) %>% 
  as.matrix()
y_train_tbl <- train_data_with_summ_stats_imp_nzv %>%
  dplyr::select(dass_dep_deteriorate_or_not) %>%
  as.matrix()
id_train_vec <-  train_data_with_summ_stats_imp_nzv %>% 
  dplyr::select(id_ema) %>% pull()

## Creating features using the saved model

best_model_weights_loaded <- load_model_hdf5("best_autoencoder_auc.h5") %>% 
  compile(
  loss = "mean_squared_error",
  optimizer = "rmsprop"
)

train_data_with_autoencoder_features_best <- predict(best_model_weights_loaded, x_train_tbl) %>% 
  as_tibble() %>% 
  bind_cols(tibble(dass_dep_deteriorate_or_not = y_train_tbl, id_ema = id_train_vec)) %>% 
  as_tibble() %>% 
  mutate(dass_dep_deteriorate_or_not = as.factor(dass_dep_deteriorate_or_not))

## Setting up code necessary to fit saved model to test data

x_test_tbl <- test_data_with_summ_stats_imp_nzv_complete %>% 
  dplyr::select(-c(id_ema,dass_dep_deteriorate_or_not)) %>% 
  as.matrix()
# y_train_tbl <- train_data_with_summ_stats_imp_nzv %>% 
#   dplyr::select(dass_dep_deteriorate_or_not) %>%
#   as.matrix()
y_test_vec <- test_data_with_summ_stats_imp_nzv_complete %>% 
  dplyr::select(dass_dep_deteriorate_or_not) %>% pull()
id_test_vec <-  test_data_with_summ_stats_imp_nzv_complete %>% 
  dplyr::select(id_ema) %>% pull()

## Creating features using the saved model

test_data_with_autoencoder_features_best <- predict(best_model_weights_loaded, x_test_tbl) %>% 
  as_tibble() %>% 
  bind_cols(tibble(dass_dep_deteriorate_or_not = y_test_vec, id_ema = id_test_vec)) %>% 
  as_tibble() %>% 
  mutate(dass_dep_deteriorate_or_not = as.factor(dass_dep_deteriorate_or_not))

```

```{r setting up random forest model for best autoencoder model with hyperparameter tuning}

##### Random Forest #####

## Now have to create the random forest engine 

rf_mod_best_auto <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

## Set up new recipe

rec_ema_neural_net_best_auto <- 
  recipes::recipe(dass_dep_deteriorate_or_not ~ ., data = train_data_with_autoencoder_features_best) %>% 
  update_role(id_ema, new_role = "id")

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_rf_wf_best_auto <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net_best_auto) %>% 
  add_model(rf_mod_best_auto)

## Now fit the model to the test data 

ema_rf_wf_fit <- fit(ema_autoencoder_rf_wf_best_auto, data = train_data_with_autoencoder_features_best)

## Evaluating performance in training set

results_train <- ema_rf_wf_fit %>%
  predict(new_data = train_data_with_autoencoder_features_best, type = "prob") %>% 
  mutate(truth = as.factor(train_data_with_autoencoder_features_best$dass_dep_deteriorate_or_not)) 

results_train %>% 
  yardstick::roc_curve(truth, .pred_1, event_level = "second") %>% 
  autoplot()

results_train %>% 
  yardstick::roc_auc(truth, .pred_1, event_level = "second")

## And in the testing set

results_test <- ema_rf_wf_fit %>%
  predict(new_data = test_data_with_autoencoder_features_best, type = "prob") %>% 
  mutate(truth = as.factor(test_data_with_autoencoder_features_best$dass_dep_deteriorate_or_not)) 

results_train %>% 
  yardstick::roc_curve(truth, .pred_1, event_level = "second") %>% 
  autoplot()

results_train %>% 
  yardstick::roc_auc(truth, .pred_1, event_level = "second")

```

```{r doing hyperparamter training for autoencoder model with max AUC and raw features model}

## Create tuning parameter set from this workflow

rf_set_autoencoder <- parameters(ema_autoencoder_rf_wf_best_auto)

## Then update the tuning set to give some bounds to mtry

rf_set_autoencoder <- 
  rf_set_autoencoder %>% 
  update(mtry = mtry(c(2L, 24L)))

## Creating folds
set.seed(33)
folds_ema_pred_best_auto <- vfold_cv(train_data_with_autoencoder_features_best, v = 4, repeats = 10, strata = dass_dep_deteriorate_or_not)

registerDoMC(cores = 7) 

## Running the workflow with the tuning

tic()
set.seed(33)
search_res_rf_autoencoder_best_auto <-
  ema_autoencoder_rf_wf_best_auto %>% 
  tune_bayes(
    resamples = folds_ema_pred_best_auto,
    # To use non-default parameter ranges
    param_info = rf_set_autoencoder,
    # Generate five at semi-random to start
    initial = 5,
    iter = 25,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 5, verbose = TRUE)
  )
toc()

## Pull the best combination of hypetparameters

best_hyp <- search_res_rf_autoencoder_best_auto %>%
  select_best("roc_auc")

## Establishing final workflow following tuning

final_wf_best_auto <- 
  ema_autoencoder_rf_wf_best_auto %>% 
  finalize_workflow(best_hyp)

## Now fit the finalized workflow on resamples

   
  ## Run the CV models here
keep_pred <- control_resamples(save_pred = TRUE)
tic()
set.seed(33)
fit_rs_final_best_auto <- 
  final_wf_best_auto %>% 
  fit_resamples(folds_ema_pred_best_auto, control = keep_pred)
toc()

## And get the metrics here

auc <- fit_rs_final_best_auto %>% 
  collect_predictions(summarize = TRUE) %>% 
  roc_auc(truth = dass_dep_deteriorate_or_not, .estimate = .pred_1) %>% 
  print()

```

```{r looking at model interpretability}

## This is a brief blog post on SHAP values in R, which look like some variation of permutation testing https://blog.datascienceheroes.com/how-to-interpret-shap-values-in-r/

## Nick Jacboson's group uses them in this paper as well https://www.nature.com/articles/s41598-021-81368-4

```

## Everything Below Here I Haven't Actually Used Yet/Won't Use Without Implementing Nested Cross Validation

```{r setting up random forest model for many models}

##### Random Forest #####

## Now have to create the random forest engine 

rf_mod <- 
  rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_rf_wf <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(rf_mod)

## Create tuning parameter set from this workflow

rf_set <- parameters(ema_autoencoder_rf_wf)

## Then update the tuning set to give some bounds to mtry

rf_set <- 
  rf_set %>% 
  update(mtry = mtry(c(2L, 24L)))
```

```{r setting up elastic net model for many models}
##### Elastic Net #####

el_net_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_el_net_wf <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(el_net_mod)

## Create tuning parameter set from this workflow

el_net_set <- parameters(ema_autoencoder_el_net_wf)

```

```{r setting up bagged mars model for many models}

## Now have to create the random forest engine 

bagged_mars_mod <- 
  bag_mars(num_terms = tune(), prod_degree = tune(), prune_method = tune()) %>% 
  set_engine("earth") %>% 
  set_mode("classification")

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_bag_mars_wf <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(bagged_mars_mod)

## Create tuning parameter set from this workflow

bagged_mars_set <- parameters(ema_autoencoder_bag_mars_wf)

```

```{r setting up bagged decision tree model for many models}

## Now have to create the random forest engine 

bagged_tree_mod <- 
  bag_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_bag_tree_wf <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(bagged_tree_mod)

## Create tuning parameter set from this workflow

bagged_tree_set <- parameters(ema_autoencoder_bag_tree_wf)

bagged_tree_set <- 
  bagged_tree_set %>% 
  update(min_n = mtry(c(2L, 19L)))

```

```{r setting up c5 rule based model for many models}

## Now have to create the random forest engine 

c5_rules_mod <- 
  C5_rules(trees = tune())

## And workflow (combining fitting the model with the preprocessing recipe)

ema_autoencoder_c5_rules_wf <- 
  workflow() %>% 
  add_recipe(rec_ema_neural_net) %>% 
  add_model(c5_rules_mod)

## Create tuning parameter set from this workflow

c5_rules_set <- parameters(ema_autoencoder_c5_rules_wf)

```

```{r function for doing hyperparameter tuning across all base models}

workflow_list <- list(ema_autoencoder_rf_wf, ema_autoencoder_el_net_wf, ema_autoencoder_bag_mars_wf)
parameter_list <- list(rf_set, el_net_set, bagged_mars_set)

finalized_workflows <- map2(.x = workflow_list, .y = parameter_list, ~{

  registerDoMC(cores = 7) 
  ## Running the workflow with the tuning
tic()
set.seed(33)
search_res <-
  .x %>% 
  tune_bayes(
    resamples = folds_ema_pred,
    # To use non-default parameter ranges
    param_info = .y,
    # Generate five at semi-random to start
    initial = 5,
    iter = 25,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 5, verbose = TRUE)
  )
toc()

## Pull the best combination of hypetparameters

best_hyp <- search_res %>%
  select_best("roc_auc")

## Establishing final workflow following tuning

final_wf <- 
  .x %>% 
  finalize_workflow(best_hyp)

return(final_wf)
  
}) 

```

```{r function for fitting many models across optimal hyperparameters}

## Now want to take the finalized workflows and get the final predictions

finalized_base_models_with_predictions <- map(finalized_workflows, ~{
 
  registerDoMC(cores = 7) 
   
  ## Run the CV models here
keep_pred <- control_resamples(save_pred = TRUE)
tic()
set.seed(33)
fit_rs_final <- 
  .x %>% 
  fit_resamples(folds_ema_pred, control = keep_pred)
toc()

return(fit_rs_final)
  
})

```

```{r getting metrics across all the base models}

## Getting metrics from the base models

base_model_metrics <- map_df(finalized_base_models_with_predictions, ~{
  
  metrics_df <- collect_metrics(.x, summarize = TRUE)
  return(metrics_df)
  
}) %>% 
  print()

```

```{r extract base model predictions to create a new feature space}

## Now have to extract predictions from each model and create a new dataframe where the predictions, outcomes, and ID variables are all together

## First, collecting predictions from both dataframes

base_model_predictions <- map(finalized_base_models_with_predictions, ~{
  
  pred_df <- collect_predictions(.x, summarize = TRUE) %>% 
    dplyr::select(-.config)
  return(pred_df)
  
}) %>% 
  reduce(left_join, by = ".row") %>% 
  dplyr::select(row = .row, dass_dep_deteriorate_or_not = dass_dep_deteriorate_or_not.x, contains(".pred"))
  

```

```{r}

## Create a recipe for the prediction-based feature space

preds_as_feat_rec <- 
  recipe(dass_dep_deteriorate_or_not ~ ., data = base_model_predictions) %>% 
  update_role(row, new_role = "id_variable") %>% 
  step_dummy(all_nominal(), -all_outcomes())

summary(preds_as_feat_rec)

```

```{r}

## Then need to run a boosted tree model with hyperparameter tuning based on this new feature space to create final predictions

## And workflow (combining fitting the model with the preprocessing recipe)

boost_tree_mod <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") %>% 
  translate()

bt_wf <- 
  workflow() %>%
  add_model(boost_tree_mod) %>%
  add_recipe(preds_as_feat_rec)

## We need to train using cross-validation, so we set up 5-fold CV here with 5 repeats

set.seed(33)
folds_pred_as_feat <- vfold_cv(base_model_predictions, v = 4, repeats = 4, strata = dass_dep_deteriorate_or_not)

## Run the final model with resamples

## Run the CV models here
keep_pred <- control_resamples(save_pred = TRUE)
tic()
set.seed(33)
bt_fit_rs_final <- 
  bt_wf %>% 
  fit_resamples(folds_pred_as_feat, control = keep_pred)
toc()

## And get the metrics here

collect_metrics(bt_fit_rs_final, summarize = TRUE)

## Saving the predictions/actual differences here

bt_adv_res <- collect_predictions(bt_fit_rs_final, summarize = TRUE)

bt_fit_rs_final$.notes

```

## Old Not In Use Code

```{r}

test_ema_data_dtvem <- ema_data_with_dep_outcome_num_resp %>% 
  slice(1:56)

get_dtvem_lags = function(x){
  
  ## Creating the function we need to run across dataframe columns based on http://www.nicholasjacobson.com/post/digital_biomarkers_mood_disorders/

  ## Selecting all the columns we need and attempting to run the map function
  dtvem_lags_df <- x %>% 
  dplyr::select(where(is.numeric), -c(time:beepvar, id_int, number_of_responses)) %>% 
  map(~{
  
  num_lags <- length(.x) - 1
      
  x2=data.frame(outcome=NA,lag=NA,pred=NA)
  for(i in 1:num_lags){
    outcome=.x[1:(length(.x)-i)] 
    pred=.x[-1:-i]
    tempx=data.frame(outcome=outcome,lag=i,pred=pred)
    x2=rbind(x2,tempx)
    return(x2)
  }
  }
  )}

dtvem_stats <- map(test_dtvem_output_lags, ~{
  
  out=bam(outcome~s(lag,by=pred, k = 2),data=.x)
  pdat=data.frame(pred=1,lag=num_lags)
  stage1est=predict.bam(out,pdat,type="terms")
  
  tibble_dtvem_stage_1 <- as_tibble(stage1est) %>% 
  dplyr::mutate(lag = (n()-row_number()) + 1) %>% 
  arrange(lag) %>% 
  pivot_wider(
    names_from = lag,
    names_prefix = "lag_",
    names_sep = "_",
    values_from = `s(lag):pred`
  ) %>% 
  unnest()
  return(tibble_dtvem_stage_1)
})

safely_get_dtvem_lags <- safely(get_dtvem_lags)

test_dtvem_output_lags <- get_dtvem_lags(test_ema_data_dtvem)

safely_do_dtvem_stage1 <- safely(do_dtvem_stage1)

test_dtvem_output <- safely_do_dtvem_stage1(test_dtvem_output_lags)

train_data_with_dtvem_stats <- train_nested_ema_data %>% 
  mutate(ema_dtvem_stats = map(data, do_dtvem_stage1)) %>% 
  unnest(ema_spec_stats) %>%
  distinct(id_ema, .keep_all = T) %>% 
  print()

dtvem_stats_df <- test_ema_data_dtvem %>% 
  dplyr::select(where(is.numeric),-c(time:beepvar, id_int, number_of_responses))
```

```{r doing it once before writing a function to get all autocorrelations for all lags}


## Doing it once outside pipes

k <- 1:55
name <- str(ema_data_with_dep_outcome_num_resp$stress_1)
col_names <- paste0("stress_1_acf_lag_", k)

tidyverse_lags_test_reduced <- ema_data_with_dep_outcome_num_resp %>%
  slice(1:112)

tidyverse_lags_test <- tidyverse_lags_test_reduced %>% 
  dplyr::select(-c(beepvar, id_int, number_of_responses)) %>% 
  dplyr::mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  )) %>% 
  dplyr::select(stress_1, day) %>% 
    tq_mutate(
        mutate_fun = lag.xts,
        k          = 1:55,
        col_rename = col_names
    ) %>% 
  pivot_longer(
    cols = contains("lag"),
    names_to = "lag", 
    values_to = "lag_value"
  ) %>%
    group_by(lag) %>%
    summarize(
        cor = cor(x = stress_1, y = lag_value, use = "pairwise.complete.obs")
        ) %>% 
  pivot_wider(
    names_from = lag,
    values_from = cor
  ) %>% 
  print()

## Now doing it once in pipes

tidy_get_lag_acf <- function(.data, var_for_acf) {

var_name <- .data %>% 
  dplyr::select({{var_for_acf}}) %>% 
    names()
    
k_func <- 1:10
col_names_tidy <- glue("{var_name}_lag_{k_func}")

tidyverse_lags_test <- .data %>%
  dplyr::select({{var_for_acf}}, day, id_ema) %>%
  group_by(id_ema) %>% 
    tq_mutate(
        mutate_fun = lag.xts,
        k          = 1:10,
        col_rename = col_names_tidy
    ) %>% 
  pivot_longer(
    cols = contains("lag"),
    names_to = "lag", 
    values_to = "lag_value"
  ) %>%
    group_by(lag, id_ema) %>%
    dplyr::summarise(
        cor_acf = cor(x = {{var_for_acf}}, y = lag_value, use = "pairwise.complete.obs")
        ) %>% 
  pivot_wider(
    names_from = lag,
    values_from = cor_acf
  ) %>% 
  dplyr::select(-contains("id_ema"))
  
}

tidyverse_lags_test_reduced_test_one_var <- tidyverse_lags_test_reduced %>% 
  tidy_get_lag_acf(stress_1) %>% 
  print()

## Can now map through all of the columns for one id

 mapping_over_all_cols_lag_acf <- function(.x){
    
  tidyverse_lags_reduced_pipe <- lag_stats_temp_df %>%
  tidy_get_lag_acf(var_for_acf = .data[[.x]])
    
  }

## Creating function that works with nested data

 do_acf_analysis <- function(x){
  
  lag_stats_temp_df <- x %>% 
  dplyr::select(-c(beepvar, id_int, number_of_responses)) %>% 
  group_by(id_ema) %>% 
  complete(time = 1:56) %>% # Needed to make sure each person has the same number of timepoints https://aosmith.rbind.io/2018/06/27/uneven-grouped-autocorrelation/#problems-with-naively-using-acf
  ungroup() %>%
  mutate(across(where(is.numeric), ~na_interpolation(.x, option = "spline")
  ))
  
  lag_stats_names <- x %>% 
    dplyr::select(-c(time:beepvar, id_int, number_of_responses)) %>% 
    dplyr::select(where(is.numeric)) %>% 
    names()

 test_acf_all_cols_one_id <- future_map_dfc(lag_stats_names, mapping_over_all_cols_lag_acf)
  
 }
 
future::plan(multisession, workers = 7)

tic()
train_data_with_lag_stats <- train_nested_ema_data %>%
  mutate(ema_lag_stats = map(ema_data, do_acf_analysis)) %>%
  unnest(ema_lag_stats) %>%
  distinct(id_ema, .keep_all = T) %>%
  print()
toc()

```

```{r writing a function to get all autocorrelations for all lags using the function on nested ema data}

## Figured out how to do a nested map here https://stackoverflow.com/questions/46392282/map-function-to-second-level-of-nested-list-using-purrr

## Also if we want to reference the outer part of the loop (in this case making sure we use the right ema_data matched to the correct id) we need to follow this post https://stackoverflow.com/questions/48847613/purrr-map-equivalent-of-nested-for-loop

## Figured out this would actually be a 3 level map, and that feels completely bonkers so revised the function to work on the long data only including the training ids

# plan(multisession, workers = 7)
# 
# tic()
# train_data_with_lag_stats <- train_nested_ema_data %>%
#   mutate(ema_lag_stats = map(ema_data, ~map_dfc(vars_for_acf_names, ~mapping_over_all_cols_lag_acf(vars_for_func, ema_data)), ema_data = .x)) %>%
#   unnest(ema_lag_stats) %>%
#   distinct(id_ema, .keep_all = T) %>%
#   print()
# toc()
# 
# train_nested_ema_data$ema_data
# 
# plan(multisession, workers = 7)
# 
# train_data_with_lag_stats <- ema_data_with_dep_outcome_num_resp %>% 
#   filter(id_ema %in% train_nested_ema_data$id_ema) %>% 
#   mutate(ema_lag_stats = NULL) %>% 
#   nest(ema_lag_stats) %>% 
#   mutate(ema_lag_stats = map(ema_data, ~map_dfc(vars_for_acf_names, ~mapping_over_all_cols_lag_acf(.x, .y)), .y = .x))

```
